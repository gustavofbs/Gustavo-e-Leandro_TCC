\section{Problema}

Mesmo que a automação de testes seja reconhecida como essencial para manter a confiabilidade dos sistemas, a criação e manutenção de testes funcionais End-to-End (E2E) continuam sendo tarefas custosas \cite{Leotta2024}, sujeitas à fragilidade (“flaky”) e à elevada necessidade de intervenção humana. Além disso, scripts tornam-se obsoletos com frequência, e a constante atualização dos localizadores utilizados para identificar elementos da interface gráfica aumenta significativamente o esforço de manutenção imposto às equipes \cite{Pysmennyi2025}.

No contexto do desenvolvimento ágil, a utilização de especificações BDD em linguagem natural busca promover um entendimento compartilhado dos requisitos. Contudo, a elaboração desses cenários é frequentemente subjetiva, variável e dependente do estilo de escrita dos envolvidos \citeonline{Alinezhadtilaki2025}. A ausência de padronização dificulta a transformação desses artefatos em testes executáveis e amplia a lacuna entre os requisitos descritos e sua verificação automatizada. Embora modelos de linguagem tenham apresentado avanços na geração de testes a partir de requisitos textuais, ainda existem limitações quanto à executabilidade e à correta interpretação dos elementos da interface \cite{Pysmennyi2025}.

Esse cenário evidencia a ausência de diretrizes consolidadas para a escrita de cenários BDD que permitam sua interpretação automática por sistemas baseados em inteligência artificial, comprometendo a correspondência entre os passos descritos e os elementos reais da interface e reduzindo a confiabilidade dos testes gerados \cite{GUPTA2023102141}. Assim, torna-se necessário investigar como a forma de escrita desses cenários influencia a capacidade de sistemas de IA em identificar corretamente elementos da interface gráfica e gerar testes automatizados de forma precisa.